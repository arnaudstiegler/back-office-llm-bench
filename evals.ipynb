{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a1ee121-5b8c-4eec-bf73-bc3514022f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contains parsable json:  0.494\n",
      "json dict has the valid format:  1.0\n",
      "accuracy on the valid answers:  0.402834008097166\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "OPENMATH eval\n",
    "'''\n",
    "\n",
    "\n",
    "from dataset import OpenMathDataset\n",
    "import json\n",
    "from utils import find_and_parse_json\n",
    "\n",
    "\n",
    "not_json_mode_path = '/Users/arnaudstiegler/back-office-llm-bench/openai_predictions/open_math/predictions_not_json_mode.json'\n",
    "json_mode_path = '/Users/arnaudstiegler/back-office-llm-bench/openai_predictions/open_math/predictions_json_mode.json'\n",
    "\n",
    "\n",
    "preds = json.load(open(json_mode_path))\n",
    "dataset = OpenMathDataset(json_mode=False)\n",
    "contains_json = []\n",
    "correct_json = []\n",
    "correct_answer = []\n",
    "for i in range(len(preds)):\n",
    "    answer = preds[i]['choices'][0]['message']['content']\n",
    "    # Starts at 1\n",
    "    sample = dataset[i+1]\n",
    "\n",
    "    json_dict = find_and_parse_json(answer)\n",
    "\n",
    "    if not json_dict:\n",
    "        contains_json.append(0)\n",
    "    else:\n",
    "        contains_json.append(1)\n",
    "\n",
    "        if not set(json_dict.keys()) == {'answer'}:\n",
    "            correct_json.append(0)\n",
    "        else:\n",
    "            correct_json.append(1)\n",
    "\n",
    "            if str(json_dict['answer']) == str(sample.answer):\n",
    "                correct_answer.append(1)\n",
    "            else:\n",
    "                correct_answer.append(0)\n",
    "\n",
    "print('contains parsable json: ', sum(contains_json) / len(contains_json))\n",
    "print('json dict has the valid format: ', sum(correct_json) / len(correct_json))\n",
    "print('accuracy on the valid answers: ', sum(correct_answer) / len(correct_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b0615141-fa6e-47d3-bda5-defac713f1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"effective_date\": \"2010-11-04\",\n",
      "    \"jurisdiction\": \"Massachusetts\",\n",
      "    \"party\": [\"Raymond_L._D'Arcy\", \"Interactive_Data_Corporation\"],\n",
      "    \"term\": \"10_years\"\n",
      "}\n",
      "```\n",
      "======\n",
      "{\n",
      "  \"effective_date\": \"2016-07-14\",\n",
      "  \"jurisdiction\": \"California\",\n",
      "  \"party\": [\n",
      "    \"Wizard_World_Inc.\",\n",
      "    \"Randall_S._Malinoff\"\n",
      "  ],\n",
      "  \"term\": \"Employee's_period_of_employment\"\n",
      "}\n",
      "======\n",
      "{\n",
      "    \"effective_date\": \"2018-01-15\",\n",
      "    \"jurisdiction\": \"Illinois\",\n",
      "    \"party\": [\n",
      "        \"DeAnn_O'Donovan\",\n",
      "        \"AHP_Servicing_LLC\",\n",
      "        \"AHP_Capital_Management_LLC\"\n",
      "    ],\n",
      "    \"term\": \"12_months\"\n",
      "}\n",
      "======\n",
      "contains parsable json:  0.9881422924901185\n",
      "json dict has the valid format:  1.0\n",
      "accuracy on the valid answers:  0.800531914893617\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "KLEISTER NDA \n",
    "'''\n",
    "\n",
    "\n",
    "from dataset import KleisterNdaDataset\n",
    "import json\n",
    "not_json_mode_path = '/Users/arnaudstiegler/back-office-llm-bench/openai_predictions/kleister_nda/predictions_not_json_mode.json'\n",
    "json_mode_path = '/Users/arnaudstiegler/back-office-llm-bench/openai_predictions/kleister_nda/predictions_json_mode.json'\n",
    "\n",
    "\n",
    "preds = json.load(open(not_json_mode_path))\n",
    "dataset = KleisterNdaDataset(json_mode=True)\n",
    "contains_json = []\n",
    "correct_json = []\n",
    "correct_answer = []\n",
    "for i in range(len(preds)):\n",
    "    answer = preds[i]['choices'][0]['message']['content']\n",
    "    sample = preds[i]['sample']\n",
    "\n",
    "    json_dict = find_and_parse_json(answer)\n",
    "\n",
    "    if not json_dict:\n",
    "        contains_json.append(0)\n",
    "        print(answer)\n",
    "        print('======')\n",
    "    else:\n",
    "        contains_json.append(1)\n",
    "\n",
    "        # if not set(json_dict.keys()) == {'answer'}:\n",
    "        if not set(json_dict.keys()) == {\"effective_date\",\"jurisdiction\",\"party\",\"term\"}:\n",
    "            correct_json.append(0)\n",
    "        else:\n",
    "            correct_json.append(1)\n",
    "\n",
    "            # if str(json_dict['answer']) == str(sample.answer):\n",
    "            answer = json.loads(sample['answer'])\n",
    "            for key in answer.keys():\n",
    "                if isinstance(json_dict[key], str):\n",
    "                    if json_dict[key] == answer[key]:\n",
    "                # if json_dict == sample.answer:\n",
    "                        correct_answer.append(1)\n",
    "                    else:\n",
    "                        # print(json_dict[key], answer[key])\n",
    "                        correct_answer.append(0)\n",
    "                elif isinstance(json_dict[key], list):\n",
    "                    pred_set = {elem.lower().replace(\".\",\"\").replace(\",\",\"\") for elem in json_dict[key]}\n",
    "\n",
    "                    if isinstance(answer[key], str):\n",
    "                        answer[key] = [answer[key]]\n",
    "                    answer_set = {elem.lower().replace(\".\",\"\").replace(\",\",\"\") for elem in answer[key]}\n",
    "                    if pred_set == answer_set:\n",
    "                        correct_answer.append(1)\n",
    "                    else:\n",
    "                        # print(pred_set, answer_set)\n",
    "                        correct_answer.append(0)\n",
    "\n",
    "print('contains parsable json: ', sum(contains_json) / len(contains_json))\n",
    "print('json dict has the valid format: ', sum(correct_json) / len(correct_json))\n",
    "print('accuracy on the valid answers: ', sum(correct_answer) / len(correct_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "75ddba1f-3773-49e8-9e2b-8243715e007f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The headquarters of the Northern Railway in India are located in New Delhi. \n",
      "\n",
      "{\"answer\": New Delhi}\n",
      "======\n",
      "The basis of the union of nations that the Swiss have continually rejected membership in since the 1990's is the signing of the Maastricht Treaty.  \n",
      "  \n",
      "{\"answer\": Maastricht Treaty}\n",
      "======\n",
      "Arkansas is the U.S. state that produces the most rice. In Arkansas, you will find a diamond mine at the Crater of Diamonds State Park.\n",
      "\n",
      "{\"answer\": Crater of Diamonds State Park}\n",
      "======\n",
      "In September 1914, the last war won by the country where Prison Break premiered corresponds to the United States. The initial German offensive on the Western Front was stopped at the Battle of the Marne.\n",
      "\n",
      "{\"answer\": Battle of the Marne}\n",
      "======\n",
      "First step: Identify the singer who sang the song \"White Christmas\". The singer of \"White Christmas\" is Bing Crosby.\n",
      "\n",
      "Second step: Identify who sang along with Bing Crosby on the song \"Silver Bells\". The singer who sang along with Bing Crosby on \"Silver Bells\" is Rosemary Clooney.\n",
      "\n",
      "{\n",
      "    \"answer\": Rosemary Clooney\n",
      "}\n",
      "======\n",
      "Step 1: Find the movie \"Ira Madiyama.\"\n",
      "Step 2: Identify the country that released the movie.\n",
      "Step 3: Determine the passport-issuing authority in that country.\n",
      "\n",
      "The passport-issuing authority in the country that released \"Ira Madiyama\" is Sri Lanka.\n",
      "\n",
      "{\"answer\": Sri Lanka}\n",
      "======\n",
      "The character who plays the US president presiding over the legislature at the signing of the Declaration of Independence in \"Sons of Liberty\" is \"George Washington.\"\n",
      "\n",
      "```json\n",
      "{\"answer\": George Washington}\n",
      "```\n",
      "======\n",
      "The first national figure buried in the abbey by the father of Mary Cromwell, Countess Fauconberg was Oliver Cromwell.  \n",
      "\n",
      "{\"answer\": Oliver Cromwell}\n",
      "======\n",
      "Robert Gli≈Ñski was born in Warsaw, Poland. In Warsaw, the power of legislative action lies with the City Council.  \n",
      "\n",
      "{\"answer\": Warsaw City Council}\n",
      "======\n",
      "1. WFGS is located in the state of Kentucky.\n",
      "2. The name \"Kentucky Fried Chicken\" was bestowed by the founder, Harland Sanders.\n",
      "\n",
      "{\"answer\": Kentucky}\n",
      "======\n",
      "Step 1: The leader under whose leadership the empire reached its greatest extent with the term \"west\" is Genghis Khan.\n",
      "Step 2: The mother of Genghis Khan is Hoelun.\n",
      "\n",
      "{\"answer\": Hoelun}\n",
      "======\n",
      "To find the network that first aired the show where the Lahnstein family is from, we need to identify the show itself. The Lahnstein family is from the German soap opera \"Verbotene Liebe\" (Forbidden Love). The network that first aired this show is Das Erste.\n",
      "\n",
      "```json\n",
      "{\"answer\": Das Erste}\n",
      "```\n",
      "======\n",
      "1. Identify the city where Spectre filming took place: Mexico City\n",
      "2. Determine the city where he died: Rome\n",
      "3. Find the main party district in Rome: Trastevere\n",
      "\n",
      "{\"answer\": Trastevere}\n",
      "======\n",
      "The songwriter of \"Perfect\" wrote the song \"Love Yourself\" for Justin Bieber.\n",
      "\n",
      "{\"answer\": Love Yourself}\n",
      "======\n",
      "Jordin Sparks\n",
      "======\n",
      "Find the city on the Avalon Peninsula with the seventh-highest metropolitan crime index in 2010:\n",
      "\n",
      "1. Locate cities on the Avalon Peninsula in Newfoundland and Labrador, Canada.\n",
      "2. Identify the city with the seventh-highest metropolitan crime index in 2010 among these cities.\n",
      "\n",
      "**JSON Output:**\n",
      "{\"answer\": \"St. John's\"}\n",
      "======\n",
      "The former name of Song dynasty's capital is Bianjing.\n",
      "\n",
      "{\"answer\": Bianjing}\n",
      "======\n",
      "The lion of Judah in the Bible was believed to be the son of God by Jesus Christ.  \n",
      "\n",
      "{\"answer\": Jesus Christ}\n",
      "======\n",
      "Step 1: Identify the country with the only Reformed church in the world.\n",
      "Step 2: Find the most important sports event held in this country.\n",
      "\n",
      "Answer: \"Netherlands\"\n",
      "\n",
      "{\n",
      "  \"answer\": \"Football (Soccer) - Eredivisie (Netherlands' Top Football League)\"\n",
      "}\n",
      "======\n",
      "Attackers of Pearl Harbor failed to capture the aircraft carriers.\n",
      "\n",
      "{\"answer\": carriers}\n",
      "======\n",
      "Most consider the event that ended the first representative government to be the Fall of Rome.  \n",
      "\n",
      "{\"answer\": Fall of Rome}\n",
      "======\n",
      "To find the answer, I need to check the cast of the movie \"I Can Only Imagine\" to determine who played the role of the creator of \"Better Than a Hallelujah.\" Looking up the information, the actor who played the creator of \"Better Than a Hallelujah\" in the movie is **Priscilla Shirer**.\n",
      "\n",
      "```json\n",
      "{\"answer\": Priscilla Shirer}\n",
      "```\n",
      "======\n",
      "1. Olga Havlov√° was born in Hradec Kr√°lov√©, Czechoslovakia, which is now in the Czech Republic.\n",
      "2. The castle in Hradec Kr√°lov√© is called Hradec Kr√°lov√© Castle.\n",
      "\n",
      "{\"answer\": Hradec Kr√°lov√© Castle}\n",
      "======\n",
      "Formation of the Grand Canyon began by the body of water that carved a channel through it around 6 million years ago.\n",
      "\n",
      "{\"answer\": 6 million years ago}\n",
      "======\n",
      "Vincent and the Doctor\n",
      "\n",
      "{\"answer\": Vincent and the Doctor}\n",
      "======\n",
      "NBC\n",
      "======\n",
      "Christianity became the official religion of the Roman Empire in 380 AD.\n",
      "\n",
      "{\"answer\": 380 AD}\n",
      "======\n",
      "First, identify the TV show Sliders is set in San Francisco.  \n",
      "\n",
      "Now, in the city where the TV show Sliders is set, the general strike of 1934 was initiated by the Longshoremen's Union.  \n",
      "\n",
      "```json\n",
      "{\"answer\": Longshoremen's Union}\n",
      "```\n",
      "======\n",
      "The people whose government awarded the Order of the Chrysanthemum failed to capture Korea.  \n",
      "  \n",
      "{\"answer\": Korea}\n",
      "======\n",
      "The place where Ognissanti Madonna can be found was designed by Giotto.\n",
      "\n",
      "```json\n",
      "{\"answer\":Giotto}\n",
      "```\n",
      "======\n",
      "The no smoking laws were passed in different states at different times, so the answer would vary based on the specific Eighteenth District School's state. To find the exact year when the no smoking laws were passed in Eighteenth District School's state, you would need to research or provide the name of the state to pinpoint the correct information. Without the specific state, I cannot provide an accurate answer.\n",
      "======\n",
      "1. The town WMNX is licensed in is Laurinburg.\n",
      "2. The airport in Laurinburg is Laurinburg-Maxton Airport.\n",
      "\n",
      "{\"answer\": Laurinburg-Maxton Airport}\n",
      "======\n",
      "Taylor Swift\n",
      "======\n",
      "Unsuccessful.\n",
      "\n",
      "Identify the factor that the people whose cars became more highly sought after as they were more fuel-efficient failed to capture. The answer is \"Future fuel prices.\"\n",
      "\n",
      "{\"answer\": Future fuel prices}\n",
      "======\n",
      "The country the Allies went after the area under which WINEP bundles the countries of northwest Africa surrendered in WWII is Italy.  \n",
      "\n",
      "{\"answer\": Italy}\n",
      "======\n",
      "Oldest surviving Buddhist school is Theravada.\n",
      "\n",
      "{\"answer\": Theravada}\n",
      "======\n",
      "The Soviets sent troops to invade Afghanistan.  \n",
      "\n",
      "{\"answer\": Afghanistan}\n",
      "======\n",
      "Sarah Walker  \n",
      "{\"answer\": Sarah Walker}\n",
      "======\n",
      "The actor who played the old man in the show \"Waiting on a Woman\" is Andy Griffith. Therefore, Aunt Bea was played by Frances Bavier.\n",
      "\n",
      "```json\n",
      "{\"answer\": Frances Bavier}\n",
      "```\n",
      "======\n",
      "The performer of the album \"If I Can't Dance\" is Sophie Ellis-Bextor. The lady who gave birth to Sophie Ellis-Bextor is Janet Ellis.\n",
      "\n",
      "```json\n",
      "{\"answer\": Janet Ellis}\n",
      "```\n",
      "======\n",
      "The subject studied in the city where the House of Wisdom was held was philosophy.\n",
      "\n",
      "{\"answer\": philosophy}\n",
      "======\n",
      "1. Identify the state where Christina Yang goes in Season 9 of the TV show \"Grey's Anatomy.\"\n",
      "2. Determine the population of that state in the year 1900.\n",
      "\n",
      "Therefore, the answer is: 1,311,564\n",
      "\n",
      "```json\n",
      "{\"answer\": 1,311,564}\n",
      "```\n",
      "======\n",
      "1. The city where Oak Tree Country Club is located is Edmond in the state of Oklahoma.\n",
      "2. One annual event held in Edmond is the Downtown Edmond Arts Festival.\n",
      "\n",
      "{\"answer\": Downtown Edmond Arts Festival}\n",
      "======\n",
      "Step 1: Find the show where Matthias Brandner is a character.\n",
      "Step 2: Identify the network that first aired the show.\n",
      "\n",
      "The show where Matthias Brandner is a character is \"Counterpart.\"\n",
      "The network that first aired the show \"Counterpart\" is Starz.\n",
      "\n",
      "{\n",
      "    \"answer\": Starz\n",
      "}\n",
      "======\n",
      "Iran\n",
      "======\n",
      "Eritrea was annexed by Italy.  \n",
      "\n",
      "{\"answer\": Italy}\n",
      "======\n",
      "TEHO originated in Finland. The currency in Finland prior to the Euro was the Finnish Markka (FIM).\n",
      "\n",
      "{\"answer\": Finnish Markka}\n",
      "======\n",
      "The Snake River in the state of Waseca starts in Minnesota.\n",
      "\n",
      "{\"answer\": Minnesota}\n",
      "======\n",
      "Knocks Folly is located in Georgia. The person who signed the Declaration of Independence from Georgia was Button Gwinnett.\n",
      "\n",
      "{\"answer\": Button Gwinnett}\n",
      "======\n",
      "To find out when the British empire took over the country where the Churchill Show is from, we need to determine the country of origin of the Churchill Show.\n",
      "\n",
      "The Churchill Show is from Kenya. The British empire took over Kenya in 1895.\n",
      "\n",
      "{\"answer\": Kenya}\n",
      "======\n",
      "Murray-Calloway County Airport is located in Calloway County, Kentucky. The ark that shares the state with this airport is the Ark Encounter, and it is situated in Grant County, Kentucky.\n",
      "\n",
      "```json\n",
      "{\"answer\": Grant}\n",
      "```\n",
      "======\n",
      "They failed to capture the wind. \n",
      "\n",
      "{\"answer\": wind}\n",
      "======\n",
      "The paternal grandparents of the Queen of Popular Music are from Barbados.\n",
      "\n",
      "{\"answer\": Barbados}\n",
      "======\n",
      "Antarctica is the continent found at the coldest of the two poles.\n",
      "\n",
      "{\"answer\": Antarctica}\n",
      "======\n",
      "Urtica dioica is commonly known as stinging nettle and it is found in various regions around the world. It has been observed that stinging nettle is used for various purposes including medicinal, culinary, and textile applications. However, the number of people visiting a place where Urtica dioica is found every year can vary significantly depending on the specific location, accessibility, tourism promotion, and other factors.\n",
      "\n",
      "**Answer:** Varied\n",
      "======\n",
      "Since Keedy House is located in Pennsylvania, the person who signed the Declaration of Independence from that state is Benjamin Franklin.\n",
      "\n",
      "{\"answer\": Benjamin Franklin}\n",
      "======\n",
      "1. Identify the capital of the country that AGT is from: Russia\n",
      "2. Determine the former capital of the Soviet Union: Moscow\n",
      "3. Find out when it starts to snow in Moscow: November\n",
      "\n",
      "{\"answer\": November}\n",
      "======\n",
      "NBC\n",
      "======\n",
      "Islamic armies conquered the nation of Egypt during the period of 639‚Äì642 AD.\n",
      "\n",
      "{\"answer\": 639‚Äì642 AD}\n",
      "======\n",
      "Yongle emperor was a Chinese emperor of the Ming dynasty. The mountain range that separates Kinnaur and Spiti from the country to which the Yongle emperor sent Yang Sanbao is the Himalayas.  \n",
      "\n",
      "{\"answer\": Himalayas}\n",
      "======\n",
      "First, identify the birthplace of Arvo Ojala.\n",
      "Arvo Ojala was born in the town of Ylivieska, Finland.\n",
      "\n",
      "Next, find the current mayor of Ylivieska, Finland.\n",
      "The current mayor of Ylivieska is Sami V√§h√§soini.\n",
      "\n",
      "Therefore, the answer to the question is:\n",
      "{\"answer\": Sami V√§h√§soini}\n",
      "======\n",
      "The person ordered to force a Tibetan assault into the region conquered by Yellow Tiger in the mid-17th century was Gushri Khan.  \n",
      "\n",
      "{\"answer\": Gushri Khan}\n",
      "======\n",
      "1. The colony where the Routzahn-Miller Farmstead is located was founded by German settlers.\n",
      "2. The first German settlers to arrive in that area were the Cocalico Creek Brethren.\n",
      "3. Therefore, the answer to the question is:\n",
      "{\"answer\": Cocalico Creek Brethren}\n",
      "======\n",
      "First, Bankard-Gunther Mansion is located in Delaware. Delaware was the first state to ratify the US Constitution in 1787. Therefore, the answer is the \"U.S. Government\". \n",
      "\n",
      "{\"answer\": U.S. Government\"}\n",
      "======\n",
      "1. Identify the actor who starred in the film \"The Caveman's Valentine\".  \n",
      "2. Find out the character played by the same actor in the film \"Avengers\".\n",
      "\n",
      "The actor who starred in \"The Caveman's Valentine\" is Samuel L. Jackson, who played the character Nick Fury in the film \"Avengers\".\n",
      "\n",
      "{\"answer\": Samuel L. Jackson}\n",
      "======\n",
      "The San Juan mountains are in the state of Colorado.\n",
      "  \n",
      "{\"answer\": Colorado}\n",
      "======\n",
      "The programming language that is an example of a 4GL named after is FOCUS.\n",
      "\n",
      "{\"answer\": FOCUS}\n",
      "======\n",
      "Country of origin for the band singing \"Love me, love me, say that you love me\" is Sweden.\n",
      "\n",
      "```json\n",
      "{\"answer\": Sweden}\n",
      "```\n",
      "======\n",
      "1. Identify the most-streamed song on Spotify: \"Shape of You\" by Ed Sheeran.\n",
      "2. Determine the song that Ed Sheeran wrote for Justin Bieber: \"Love Yourself.\"\n",
      "   \n",
      "{\"answer\": Love Yourself}\n",
      "======\n",
      "The character who once said \"consider the lilies of the field\" in The Passion of the Christ is Jesus.  \n",
      "\n",
      "{\"answer\": Jesus}\n",
      "======\n",
      "The border between the country that released Toss-Up and Armenia should be opened as requested.\n",
      "\n",
      "To determine the country that released Toss-Up, we need to consider that \"Toss-Up\" is a game show term and the show Renegade released a \"Toss-Up\" challenge. The Renegade TV show was released by the USA.\n",
      "\n",
      "Therefore, the answer to the question is:\n",
      "{\"answer\": USA}\n",
      "======\n",
      "contains parsable json:  0.858\n",
      "json dict has the valid format:  1.0\n",
      "accuracy on the valid answers:  0.32400932400932403\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "MultiHopQA \n",
    "'''\n",
    "\n",
    "\n",
    "from dataset import MultiHopQADataset\n",
    "import json\n",
    "from utils import find_and_parse_json\n",
    "not_json_mode_path = '/Users/arnaudstiegler/back-office-llm-bench/openai_predictions/multi_hop_qa/predictions_not_json_mode.json'\n",
    "json_mode_path = '/Users/arnaudstiegler/back-office-llm-bench/openai_predictions/multi_hop_qa/predictions_json_mode.json'\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "SYS_PROMPT = (\n",
    "    \"You are an AI agent used for automation. Do not act like a chatbot. Execute the task and\"\n",
    "    \"follow the instructions for the formatting of the output.\"\n",
    ")\n",
    "client = OpenAI()\n",
    "\n",
    "json_mode = False\n",
    "preds = json.load(open(not_json_mode_path if json_mode else not_json_mode_path))\n",
    "dataset = MultiHopQADataset(json_mode=False)\n",
    "contains_json = []\n",
    "correct_json = []\n",
    "correct_answer = []\n",
    "for i in range(len(preds)):\n",
    "    answer = preds[i]['choices'][0]['message']['content']\n",
    "    sample = preds[i]['sample']\n",
    "\n",
    "    json_dict = find_and_parse_json(answer)\n",
    "\n",
    "    if not json_dict:\n",
    "        contains_json.append(0)\n",
    "        print(answer)\n",
    "        print('======')\n",
    "    else:\n",
    "        contains_json.append(1)\n",
    "\n",
    "        # if not set(json_dict.keys()) == {'answer'}:\n",
    "        if not set(json_dict.keys()) == {\"answer\"}:\n",
    "            correct_json.append(0)\n",
    "        else:\n",
    "            correct_json.append(1)\n",
    "\n",
    "            prompt = (f'first answer: {sample[\"answer\"]}, second answer: {json_dict[\"answer\"]}. Are those 2 answers refering to the same thing? '\n",
    "            'The two answers do not need to be exactly the same but they need to point to the same information.'\n",
    "            'For instance, if the 2 answers are date but one is just the year and the other is the full date, still count it as true.'\n",
    "            'If one answer is only a subset of the other one, you can count it as true as well.'\n",
    "            'Otherwise, formatting also do not matter when it comes to comparing answers, it just matter what the info is and not how it is formatted'\n",
    "            'Answer either yes or no and nothing else. Just yes or no')\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                # response_format={\"type\": \"json_object\"},\n",
    "            )\n",
    "            if 'yes' in response.choices[0].message.content.lower():\n",
    "                correct_answer.append(1)\n",
    "            elif 'no' in response.choices[0].message.content.lower():\n",
    "                correct_answer.append(0)\n",
    "                # print(json_dict['answer'], sample['answer'])\n",
    "            else:\n",
    "                import ipdb; ipdb.set_trace()\n",
    "    \n",
    "\n",
    "print('contains parsable json: ', sum(contains_json) / len(contains_json))\n",
    "print('json dict has the valid format: ', sum(correct_json) / len(correct_json))\n",
    "print('accuracy on the valid answers: ', sum(correct_answer) / len(correct_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb567182-b8fa-426f-8216-575e4e75ab7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8d7921f-4e3f-4812-8a32-312629f9f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer=\"{'effective_date': '2001-04-18', 'jurisdiction': 'Oregon', 'party': ['Eric_Dean_Sprunk', 'Nike_Inc.']}\"\n",
    "\n",
    "prompt = '''\n",
    "The text above is the transcription of an NDA. \n",
    "\n",
    "# Extraction:\n",
    "There are up to 6 attributes to be extracted from the transcription:\n",
    "\n",
    "effective_date - date in YYYY-MM-DD format, at which point the contract is legally binding,\n",
    "jurisdiction - under which state or country jurisdiction is the contract signed,\n",
    "party - party or parties of the contract,\n",
    "term - length of the legal contract as expressed in the document.\n",
    "Note that party usually occur more than once.\n",
    "\n",
    "# Normalization:\n",
    "The expected pieces of information were normalized to some degree:\n",
    "\n",
    "in attribute values, all spaces   and colons : were replaced with an underscores _,\n",
    "all expected dates should be returned in YYYY-MM-DD format,\n",
    "values for attribute term are normalized with the same original units e.g. eleven months is changed to 11_months; all of them are in the same format: {number}_{units}.\n",
    "For jurisdiction, only return the state name and nothing else. For instance, return \"California\" instead of \"State_of_California\"\n",
    "\n",
    "# Output Format:\n",
    "The output has to be a valid json with the following format:\n",
    "{\"effective_date\": \"value\", \"jurisdiction\":\"value\", \"party\":[\"value_1\", \"value_2\", ...], \"term\":\"value\"}\n",
    "\n",
    "So for instance:\n",
    "\"{'effective_date': '2020-01-12', 'jurisdiction': 'Utah', 'party': ['Bill_Gates', 'Coca_Cola_Inc.']}\"\n",
    "\n",
    "You can reason and explain your choices before returning the JSON object, you just have to have that json object in the output\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a0606a0-2ccd-4769-99dd-0a762217c6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import MultiHopQADataset\n",
    "\n",
    "dataset = MultiHopQADataset(json_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b762c95d-ccc9-48ac-8ed0-67180710854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "SYS_PROMPT = (\n",
    "    \"You are an AI agent used for automation. Do not act like a chatbot. Execute the task and\"\n",
    "    \"follow the instructions for the formatting of the output as a JSON object.\"\n",
    ")\n",
    "client = OpenAI(api_key=api_key)\n",
    "response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": dataset[1].prompt},\n",
    "        ],\n",
    "        # response_format={\"type\": \"json_object\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b3460cb-c363-419c-8eed-2f73babeec9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Not applicable'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_and_parse_json(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "235133e5-68be-41e6-9621-9f15c17f8823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- The group that Bush declared war against is Al-Qaeda.\\n- The country with the world\\'s largest economy in 2014 was the United States.\\n- The biggest terrorist attack by Al-Qaeda in the United States in 2014 was not applicable as there were no major attacks by Al-Qaeda in the United States during that year.\\n\\n```json\\n{\"answer\": \"Not applicable\"}\\n``` '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "210ebcb8-f160-4568-a9af-7433ad7b598e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the 9/11 attacks'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1].answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d26ad2a6-1828-4645-9d49-848a6a3615fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What were the biggest terrorist attacks by the group Bush declared war against in the country with the world\\'s largest economy in 2014?  \\n  \\n        Answer the question and return a JSON object with a key \"answer\" and the value being the answer\\n        to the question. Do not put any explanation of any sort in the Json object, only the actual\\n        answer to the question.\\n        \\n        For instance, if the question is: what is the capital of the country with the largest GDP in the world?\\n        The output would be:\\n        {\"answer\": \"USA\"}\\n        Typically, the answer should only be a few words and not a sentence.\\n        \\n         Think step by step. First write the reasoning to get to the answer, then write the Json object containing only the answer'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e87cfe5d-07d1-4319-a630-b7343d6bb35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['multihop', 'meta_info', 'final_answer', 'question', 'answer', 'tag', 'sub_questions'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'multihop': True\n",
    "# 'question': 'What is the dosage for Advil?',\n",
    "# 'unanswerable': None\n",
    "# 'answer': 'Molotov -- Ribbentrop Pact of 1939'\n",
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "92bdf597-77a9-4039-9da1-674369e119bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [sample for sample in dataset['train'] if sample['answer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "57906ed8-9baf-4cfe-8882-1625dd25fcea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6976"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Reason and answer the question above.\n",
    "The return should be a json object, with one key \"answer\" and the corresponding value should be the answer to the question.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c2b6d83f-ea12-480b-a62a-c33435e59f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'multihop': True,\n",
       " 'meta_info': {'attribute_1': None,\n",
       "  'attribute_2': None,\n",
       "  'category': None,\n",
       "  'comparison_attribute': None,\n",
       "  'entity': None,\n",
       "  'entity_1': None,\n",
       "  'entity_2': None,\n",
       "  'index': None,\n",
       "  'list_of_attributes': None,\n",
       "  'llm': 'wizard_lm',\n",
       "  'negatives': None,\n",
       "  'selected': None,\n",
       "  'slot_dic': None,\n",
       "  'src': 'un_musicque'},\n",
       " 'final_answer': 'Summary: The knowledge provided does not contain any information about the expansion of a single reed\\'s role or any composer who might have done so. There is no mention of any piece of music that is used as a clich√© to convey refinement in relation to this topic. The knowledge only pertains to Thomas & Friends and Far East Movement\\'s song \"The Illest.\" Without any information about the composer or the piece of music in question, it is impossible to generate a reasoning to answer this question. The knowledge provided does not offer any context or hints that could lead to a potential answer.\\nAnswer: I\\'m sorry, but I cannot answer this question based on the knowledge provided.',\n",
       " 'question': \"What piece, by the composer who expanded the single reeds' role, is used as a clich√© to convey refinement?\",\n",
       " 'answer': 'Eine kleine Nachtmusik',\n",
       " 'tag': 'un_musique-train.json',\n",
       " 'sub_questions': [{'answer': None,\n",
       "   'extra_info': None,\n",
       "   'long_answer': 'The question is about the expansion of a single reed\\'s role, which seems to be unrelated to the information provided about Thomas & Friends and Far East Movement\\'s song \"The Illest.\" There is no direct connection or contextual hint in the given knowledge that would allow me to answer this question.\\nAnswer: I\\'m sorry, but I cannot answer this question based on the knowledge provided.',\n",
       "   'paragraph': 'Thomas & Friends. Originally, narrating was used as the only voice in the series until 2008. Britt Allcroft thought it essential to convey the episode as a story that would be read from a book at home. Individual voice - over actors were given to both the UK and US dubs of the Series, following the switch to full CGI animation in 2009. The Illest. \"The Illest\" is a song by American hip hop group Far East Movement. The song was co-written and produced by Norwegian music producer and songwriter Axident and Wallpaper. frontman Ricky Reed, and features a guest appearance from American rapper Riff Raff. It was released digitally as a single on July 2, 2013 and has since peaked at number 18 on the US \"Billboard\" Hot Rap Songs. \"The Illest\" was featured as a bonus track and a single from the 2013 Special Edition release of their fourth studio album \"Dirty Bass\" (becoming the album\\'s fifth single overall). ',\n",
       "   'question': 'Who expanded the single reeds role?',\n",
       "   'unanswerable': True}]}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5e9dcbc0-f902-465c-947a-c86bde98decc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What piece, by the composer who expanded the single reeds' role, is used as a clich√© to convey refinement?\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[3]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53385292-5376-4b5f-9f02-d4ff3a92ba7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
