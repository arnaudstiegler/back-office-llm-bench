{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a1ee121-5b8c-4eec-bf73-bc3514022f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contains parsable json:  0.494\n",
      "json dict has the valid format:  1.0\n",
      "accuracy on the valid answers:  0.402834008097166\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "OPENMATH eval\n",
    "'''\n",
    "\n",
    "\n",
    "from dataset import OpenMathDataset\n",
    "import json\n",
    "from utils import find_and_parse_json\n",
    "\n",
    "\n",
    "not_json_mode_path = '/Users/arnaudstiegler/back-office-llm-bench/openai_predictions/open_math/predictions_not_json_mode.json'\n",
    "json_mode_path = '/Users/arnaudstiegler/back-office-llm-bench/openai_predictions/open_math/predictions_json_mode.json'\n",
    "\n",
    "\n",
    "preds = json.load(open(json_mode_path))\n",
    "dataset = OpenMathDataset(json_mode=False)\n",
    "contains_json = []\n",
    "correct_json = []\n",
    "correct_answer = []\n",
    "for i in range(len(preds)):\n",
    "    answer = preds[i]['choices'][0]['message']['content']\n",
    "    # Starts at 1\n",
    "    sample = dataset[i+1]\n",
    "\n",
    "    json_dict = find_and_parse_json(answer)\n",
    "\n",
    "    if not json_dict:\n",
    "        contains_json.append(0)\n",
    "    else:\n",
    "        contains_json.append(1)\n",
    "\n",
    "        if not set(json_dict.keys()) == {'answer'}:\n",
    "            correct_json.append(0)\n",
    "        else:\n",
    "            correct_json.append(1)\n",
    "\n",
    "            if str(json_dict['answer']) == str(sample.answer):\n",
    "                correct_answer.append(1)\n",
    "            else:\n",
    "                correct_answer.append(0)\n",
    "\n",
    "print('contains parsable json: ', sum(contains_json) / len(contains_json))\n",
    "print('json dict has the valid format: ', sum(correct_json) / len(correct_json))\n",
    "print('accuracy on the valid answers: ', sum(correct_answer) / len(correct_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0615141-fa6e-47d3-bda5-defac713f1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contains parsable json:  0.9920948616600791\n",
      "json dict has the valid format:  1.0\n",
      "accuracy on the valid answers:  0.8\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "KLEISTER NDA \n",
    "'''\n",
    "\n",
    "\n",
    "from dataset import KleisterNdaDataset\n",
    "import json\n",
    "not_json_mode_path = '/Users/arnaudstiegler/back-office-llm-bench/openai_predictions/kleister_nda/predictions_not_json_mode.json'\n",
    "json_mode_path = '/Users/arnaudstiegler/back-office-llm-bench/openai_predictions/kleister_nda/predictions_json_mode.json'\n",
    "\n",
    "\n",
    "preds = json.load(open(json_mode_path))\n",
    "dataset = KleisterNdaDataset(json_mode=True)\n",
    "contains_json = []\n",
    "correct_json = []\n",
    "correct_answer = []\n",
    "for i in range(len(preds)):\n",
    "    answer = preds[i]['choices'][0]['message']['content']\n",
    "    sample = preds[i]['sample']\n",
    "\n",
    "    json_dict = find_and_parse_json(answer)\n",
    "\n",
    "    if not json_dict:\n",
    "        contains_json.append(0)\n",
    "    else:\n",
    "        contains_json.append(1)\n",
    "\n",
    "        # if not set(json_dict.keys()) == {'answer'}:\n",
    "        if not set(json_dict.keys()) == {\"effective_date\",\"jurisdiction\",\"party\",\"term\"}:\n",
    "            correct_json.append(0)\n",
    "        else:\n",
    "            correct_json.append(1)\n",
    "\n",
    "            # if str(json_dict['answer']) == str(sample.answer):\n",
    "            answer = json.loads(sample['answer'])\n",
    "            for key in answer.keys():\n",
    "                if isinstance(json_dict[key], str):\n",
    "                    if json_dict[key] == answer[key]:\n",
    "                # if json_dict == sample.answer:\n",
    "                        correct_answer.append(1)\n",
    "                    else:\n",
    "                        # print(json_dict[key], answer[key])\n",
    "                        correct_answer.append(0)\n",
    "                elif isinstance(json_dict[key], list):\n",
    "                    pred_set = {elem.lower().replace(\".\",\"\").replace(\",\",\"\") for elem in json_dict[key]}\n",
    "\n",
    "                    if isinstance(answer[key], str):\n",
    "                        answer[key] = [answer[key]]\n",
    "                    answer_set = {elem.lower().replace(\".\",\"\").replace(\",\",\"\") for elem in answer[key]}\n",
    "                    if pred_set == answer_set:\n",
    "                        correct_answer.append(1)\n",
    "                    else:\n",
    "                        # print(pred_set, answer_set)\n",
    "                        correct_answer.append(0)\n",
    "\n",
    "print('contains parsable json: ', sum(contains_json) / len(contains_json))\n",
    "print('json dict has the valid format: ', sum(correct_json) / len(correct_json))\n",
    "print('accuracy on the valid answers: ', sum(correct_answer) / len(correct_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75ddba1f-3773-49e8-9e2b-8243715e007f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contains parsable json:  0.984\n",
      "json dict has the valid format:  1.0\n",
      "accuracy on the valid answers:  0.34552845528455284\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "MultiHopQA \n",
    "'''\n",
    "\n",
    "\n",
    "from dataset import MultiHopQADataset\n",
    "import json\n",
    "from utils import find_and_parse_json\n",
    "not_json_mode_path = '/Users/arnaudstiegler/back-office-llm-bench/openai_predictions/multi_hop_qa/predictions_not_json_mode.json'\n",
    "json_mode_path = '/Users/arnaudstiegler/back-office-llm-bench/openai_predictions/multi_hop_qa/predictions_json_mode.json'\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "SYS_PROMPT = (\n",
    "    \"You are an AI agent used for automation. Do not act like a chatbot. Execute the task and\"\n",
    "    \"follow the instructions for the formatting of the output.\"\n",
    ")\n",
    "client = OpenAI()\n",
    "\n",
    "json_mode = False\n",
    "preds = json.load(open(json_mode_path if json_mode else not_json_mode_path))\n",
    "dataset = MultiHopQADataset(json_mode=False)\n",
    "contains_json = []\n",
    "correct_json = []\n",
    "correct_answer = []\n",
    "for i in range(len(preds)):\n",
    "    answer = preds[i]['choices'][0]['message']['content']\n",
    "    sample = preds[i]['sample']\n",
    "\n",
    "    json_dict = find_and_parse_json(answer)\n",
    "\n",
    "    if not json_dict:\n",
    "        contains_json.append(0)\n",
    "        # print(answer)\n",
    "        # print('======')\n",
    "    else:\n",
    "        contains_json.append(1)\n",
    "\n",
    "        # if not set(json_dict.keys()) == {'answer'}:\n",
    "        if not set(json_dict.keys()) == {\"answer\"}:\n",
    "            correct_json.append(0)\n",
    "        else:\n",
    "            correct_json.append(1)\n",
    "\n",
    "            prompt = (f'first answer: {sample[\"answer\"]}, second answer: {json_dict[\"answer\"]}. Are those 2 answers refering to the same thing? '\n",
    "            'The two answers do not need to be exactly the same but they need to point to the same information.'\n",
    "            'For instance, if the 2 answers are date but one is just the year and the other is the full date, still count it as true.'\n",
    "            'If one answer is only a subset of the other one, you can count it as true as well.'\n",
    "            'Otherwise, formatting also do not matter when it comes to comparing answers, it just matter what the info is and not how it is formatted'\n",
    "            'Answer either yes or no and nothing else. Just yes or no')\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                # response_format={\"type\": \"json_object\"},\n",
    "            )\n",
    "            if 'yes' in response.choices[0].message.content.lower():\n",
    "                correct_answer.append(1)\n",
    "            elif 'no' in response.choices[0].message.content.lower():\n",
    "                correct_answer.append(0)\n",
    "                # print(json_dict['answer'], sample['answer'])\n",
    "            else:\n",
    "                import ipdb; ipdb.set_trace()\n",
    "    \n",
    "\n",
    "print('contains parsable json: ', sum(contains_json) / len(contains_json))\n",
    "print('json dict has the valid format: ', sum(correct_json) / len(correct_json))\n",
    "print('accuracy on the valid answers: ', sum(correct_answer) / len(correct_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb567182-b8fa-426f-8216-575e4e75ab7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8d7921f-4e3f-4812-8a32-312629f9f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer=\"{'effective_date': '2001-04-18', 'jurisdiction': 'Oregon', 'party': ['Eric_Dean_Sprunk', 'Nike_Inc.']}\"\n",
    "\n",
    "prompt = '''\n",
    "The text above is the transcription of an NDA. \n",
    "\n",
    "# Extraction:\n",
    "There are up to 6 attributes to be extracted from the transcription:\n",
    "\n",
    "effective_date - date in YYYY-MM-DD format, at which point the contract is legally binding,\n",
    "jurisdiction - under which state or country jurisdiction is the contract signed,\n",
    "party - party or parties of the contract,\n",
    "term - length of the legal contract as expressed in the document.\n",
    "Note that party usually occur more than once.\n",
    "\n",
    "# Normalization:\n",
    "The expected pieces of information were normalized to some degree:\n",
    "\n",
    "in attribute values, all spaces   and colons : were replaced with an underscores _,\n",
    "all expected dates should be returned in YYYY-MM-DD format,\n",
    "values for attribute term are normalized with the same original units e.g. eleven months is changed to 11_months; all of them are in the same format: {number}_{units}.\n",
    "For jurisdiction, only return the state name and nothing else. For instance, return \"California\" instead of \"State_of_California\"\n",
    "\n",
    "# Output Format:\n",
    "The output has to be a valid json with the following format:\n",
    "{\"effective_date\": \"value\", \"jurisdiction\":\"value\", \"party\":[\"value_1\", \"value_2\", ...], \"term\":\"value\"}\n",
    "\n",
    "So for instance:\n",
    "\"{'effective_date': '2020-01-12', 'jurisdiction': 'Utah', 'party': ['Bill_Gates', 'Coca_Cola_Inc.']}\"\n",
    "\n",
    "You can reason and explain your choices before returning the JSON object, you just have to have that json object in the output\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a0606a0-2ccd-4769-99dd-0a762217c6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import MultiHopQADataset\n",
    "\n",
    "dataset = MultiHopQADataset(json_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8f957ee-9876-40d6-b79c-baf98d8607bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What were the biggest terrorist attacks by the group Bush declared war against in the country with the world\\'s largest economy in 2014?  \\n  \\n        Answer the question and return a JSON object with a key \"answer\" and the value being the answer\\n        to the question. Do not put any explanation of any sort in the Json object, only the actual\\n        answer to the question.\\n        \\n        For instance, if the question is: what is the capital of the country with the largest GDP in the world?\\n        The output would be:\\n        {\"answer\": \"USA\"}\\n        Typically, the answer should only be a few words and not a sentence.\\n        Also, do not put \" or \\' in the value of the JSON because that prevents me from parsing the json within the string.\\n         Think step by step. First write the reasoning to get to the answer, then write the Json object containing only the answer'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b762c95d-ccc9-48ac-8ed0-67180710854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "SYS_PROMPT = (\n",
    "    \"You are an AI agent used for automation. Do not act like a chatbot. Execute the task and\"\n",
    "    \"follow the instructions for the formatting of the output as a JSON object.\"\n",
    ")\n",
    "client = OpenAI(api_key=api_key)\n",
    "response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": dataset[1].prompt},\n",
    "        ],\n",
    "        # response_format={\"type\": \"json_object\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b3460cb-c363-419c-8eed-2f73babeec9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Not applicable'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_and_parse_json(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "235133e5-68be-41e6-9621-9f15c17f8823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- The group that Bush declared war against is Al-Qaeda.\\n- The country with the world\\'s largest economy in 2014 was the United States.\\n- The biggest terrorist attack by Al-Qaeda in the United States in 2014 was not applicable as there were no major attacks by Al-Qaeda in the United States during that year.\\n\\n```json\\n{\"answer\": \"Not applicable\"}\\n``` '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "210ebcb8-f160-4568-a9af-7433ad7b598e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the 9/11 attacks'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1].answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d26ad2a6-1828-4645-9d49-848a6a3615fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What were the biggest terrorist attacks by the group Bush declared war against in the country with the world\\'s largest economy in 2014?  \\n  \\n        Answer the question and return a JSON object with a key \"answer\" and the value being the answer\\n        to the question. Do not put any explanation of any sort in the Json object, only the actual\\n        answer to the question.\\n        \\n        For instance, if the question is: what is the capital of the country with the largest GDP in the world?\\n        The output would be:\\n        {\"answer\": \"USA\"}\\n        Typically, the answer should only be a few words and not a sentence.\\n        \\n         Think step by step. First write the reasoning to get to the answer, then write the Json object containing only the answer'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e87cfe5d-07d1-4319-a630-b7343d6bb35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['multihop', 'meta_info', 'final_answer', 'question', 'answer', 'tag', 'sub_questions'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'multihop': True\n",
    "# 'question': 'What is the dosage for Advil?',\n",
    "# 'unanswerable': None\n",
    "# 'answer': 'Molotov -- Ribbentrop Pact of 1939'\n",
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "92bdf597-77a9-4039-9da1-674369e119bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [sample for sample in dataset['train'] if sample['answer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "57906ed8-9baf-4cfe-8882-1625dd25fcea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6976"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Reason and answer the question above.\n",
    "The return should be a json object, with one key \"answer\" and the corresponding value should be the answer to the question.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c2b6d83f-ea12-480b-a62a-c33435e59f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'multihop': True,\n",
       " 'meta_info': {'attribute_1': None,\n",
       "  'attribute_2': None,\n",
       "  'category': None,\n",
       "  'comparison_attribute': None,\n",
       "  'entity': None,\n",
       "  'entity_1': None,\n",
       "  'entity_2': None,\n",
       "  'index': None,\n",
       "  'list_of_attributes': None,\n",
       "  'llm': 'wizard_lm',\n",
       "  'negatives': None,\n",
       "  'selected': None,\n",
       "  'slot_dic': None,\n",
       "  'src': 'un_musicque'},\n",
       " 'final_answer': 'Summary: The knowledge provided does not contain any information about the expansion of a single reed\\'s role or any composer who might have done so. There is no mention of any piece of music that is used as a cliché to convey refinement in relation to this topic. The knowledge only pertains to Thomas & Friends and Far East Movement\\'s song \"The Illest.\" Without any information about the composer or the piece of music in question, it is impossible to generate a reasoning to answer this question. The knowledge provided does not offer any context or hints that could lead to a potential answer.\\nAnswer: I\\'m sorry, but I cannot answer this question based on the knowledge provided.',\n",
       " 'question': \"What piece, by the composer who expanded the single reeds' role, is used as a cliché to convey refinement?\",\n",
       " 'answer': 'Eine kleine Nachtmusik',\n",
       " 'tag': 'un_musique-train.json',\n",
       " 'sub_questions': [{'answer': None,\n",
       "   'extra_info': None,\n",
       "   'long_answer': 'The question is about the expansion of a single reed\\'s role, which seems to be unrelated to the information provided about Thomas & Friends and Far East Movement\\'s song \"The Illest.\" There is no direct connection or contextual hint in the given knowledge that would allow me to answer this question.\\nAnswer: I\\'m sorry, but I cannot answer this question based on the knowledge provided.',\n",
       "   'paragraph': 'Thomas & Friends. Originally, narrating was used as the only voice in the series until 2008. Britt Allcroft thought it essential to convey the episode as a story that would be read from a book at home. Individual voice - over actors were given to both the UK and US dubs of the Series, following the switch to full CGI animation in 2009. The Illest. \"The Illest\" is a song by American hip hop group Far East Movement. The song was co-written and produced by Norwegian music producer and songwriter Axident and Wallpaper. frontman Ricky Reed, and features a guest appearance from American rapper Riff Raff. It was released digitally as a single on July 2, 2013 and has since peaked at number 18 on the US \"Billboard\" Hot Rap Songs. \"The Illest\" was featured as a bonus track and a single from the 2013 Special Edition release of their fourth studio album \"Dirty Bass\" (becoming the album\\'s fifth single overall). ',\n",
       "   'question': 'Who expanded the single reeds role?',\n",
       "   'unanswerable': True}]}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5e9dcbc0-f902-465c-947a-c86bde98decc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What piece, by the composer who expanded the single reeds' role, is used as a cliché to convey refinement?\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[3]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53385292-5376-4b5f-9f02-d4ff3a92ba7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
