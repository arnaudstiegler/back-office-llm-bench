{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a1ee121-5b8c-4eec-bf73-bc3514022f62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contains parsable json:  0.997\n",
      "json dict has the valid format:  1.0\n",
      "accuracy on the valid answers:  0.765295887662989\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "OPENMATH eval\n",
    "'''\n",
    "\n",
    "\n",
    "from dataset import OpenMathDataset\n",
    "import json\n",
    "from utils import find_and_parse_json\n",
    "\n",
    "\n",
    "not_json_mode_path = '/Users/arnaudstiegler/back-office-llm-bench/openai_predictions/open_math/predictions_not_json_mode.json'\n",
    "json_mode_path = '/Users/arnaudstiegler/back-office-llm-bench/openai_predictions/open_math/predictions_json_mode.json'\n",
    "\n",
    "\n",
    "preds = json.load(open(not_json_mode_path))\n",
    "dataset = OpenMathDataset(json_mode=False)\n",
    "contains_json = []\n",
    "correct_json = []\n",
    "correct_answer = []\n",
    "for i in range(len(preds)):\n",
    "    answer = preds[i]['choices'][0]['message']['content']\n",
    "    # Starts at 1\n",
    "    sample = dataset[i+1]\n",
    "\n",
    "    json_dict = find_and_parse_json(answer)\n",
    "\n",
    "    if not json_dict:\n",
    "        contains_json.append(0)\n",
    "        # print(answer)\n",
    "        # print('======')\n",
    "    else:\n",
    "        contains_json.append(1)\n",
    "\n",
    "        if not set(json_dict.keys()) == {'answer'}:\n",
    "            correct_json.append(0)\n",
    "        else:\n",
    "            correct_json.append(1)\n",
    "\n",
    "            if str(json_dict['answer']) == str(sample.answer):\n",
    "                correct_answer.append(1)\n",
    "            else:\n",
    "                correct_answer.append(0)\n",
    "\n",
    "print('contains parsable json: ', sum(contains_json) / len(contains_json))\n",
    "print('json dict has the valid format: ', sum(correct_json) / len(correct_json))\n",
    "print('accuracy on the valid answers: ', sum(correct_answer) / len(correct_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0615141-fa6e-47d3-bda5-defac713f1ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contains parsable json:  0.9960474308300395\n",
      "json dict has the valid format:  0.996031746031746\n",
      "accuracy on the valid answers:  0.376984126984127\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "KLEISTER NDA \n",
    "'''\n",
    "\n",
    "\n",
    "from dataset import KleisterNdaDataset\n",
    "import json\n",
    "not_json_mode_path = '/Users/arnaudstiegler/back-office-llm-bench/openai_predictions/kleister_nda/predictions_not_json_mode.json'\n",
    "json_mode_path = '/Users/arnaudstiegler/back-office-llm-bench/openai_predictions/kleister_nda/predictions_json_mode.json'\n",
    "\n",
    "\n",
    "preds = json.load(open(json_mode_path))\n",
    "dataset = KleisterNdaDataset(json_mode=False)\n",
    "contains_json = []\n",
    "correct_json = []\n",
    "correct_answer = []\n",
    "for i in range(len(preds)):\n",
    "    answer = preds[i]['choices'][0]['message']['content']\n",
    "    sample = dataset[i]\n",
    "\n",
    "    json_dict = find_and_parse_json(answer)\n",
    "\n",
    "    if not json_dict:\n",
    "        contains_json.append(0)\n",
    "        # print(answer)\n",
    "        # print('======')\n",
    "    else:\n",
    "        contains_json.append(1)\n",
    "\n",
    "        # if not set(json_dict.keys()) == {'answer'}:\n",
    "        if not set(json_dict.keys()) == {\"effective_date\",\"jurisdiction\",\"party\",\"term\"}:\n",
    "            correct_json.append(0)\n",
    "        else:\n",
    "            correct_json.append(1)\n",
    "\n",
    "            # if str(json_dict['answer']) == str(sample.answer):\n",
    "            answer = json.loads(sample.answer)\n",
    "            for key in answer.keys():\n",
    "                if isinstance(json_dict[key], str):\n",
    "                    if json_dict[key] == answer[key]:\n",
    "                # if json_dict == sample.answer:\n",
    "                        correct_answer.append(1)\n",
    "                    else:\n",
    "                        # print(json_dict[key], answer[key])\n",
    "                        correct_answer.append(0)\n",
    "                elif isinstance(json_dict[key], list):\n",
    "                    if set(json_dict[key]) == set(answer[key]):\n",
    "                        correct_answer.append(1)\n",
    "                    else:\n",
    "                        # print(json_dict[key], answer[key])\n",
    "                        correct_answer.append(0)\n",
    "\n",
    "print('contains parsable json: ', sum(contains_json) / len(contains_json))\n",
    "print('json dict has the valid format: ', sum(correct_json) / len(correct_json))\n",
    "print('accuracy on the valid answers: ', sum(correct_answer) / len(correct_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "75ddba1f-3773-49e8-9e2b-8243715e007f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"answer\": \"St. John's\"}\n",
      "======\n"
     ]
    },
    {
     "ename": "AuthenticationError",
     "evalue": "Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-pGOO6***************************************OhIK. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAuthenticationError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m correct_json\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     45\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfirst answer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, second answer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjson_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manswer\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Are those 2 answers refering to the same thing? Answer either yes or no and nothing else. Just yes or no\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 46\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgpt-3.5-turbo\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msystem\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mSYS_PROMPT\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcontent\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# response_format={\"type\": \"json_object\"},\u001b[39;49;00m\n\u001b[1;32m     53\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124myes\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     55\u001b[0m     correct_answer\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/ml_framework_311/lib/python3.11/site-packages/openai/_utils/_utils.py:301\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    299\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    300\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/ml_framework_311/lib/python3.11/site-packages/openai/resources/chat/completions.py:598\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, frequency_penalty, function_call, functions, logit_bias, max_tokens, n, presence_penalty, response_format, seed, stop, stream, temperature, tool_choice, tools, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    596\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    597\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[0;32m--> 598\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    599\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    600\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    601\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    602\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    603\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    624\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    625\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    626\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    629\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/ml_framework_311/lib/python3.11/site-packages/openai/_base_client.py:1096\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1084\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1091\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1092\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1093\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1094\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1095\u001b[0m     )\n\u001b[0;32m-> 1096\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/ml_framework_311/lib/python3.11/site-packages/openai/_base_client.py:856\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    847\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequest\u001b[39m(\n\u001b[1;32m    848\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    849\u001b[0m     cast_to: Type[ResponseT],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    854\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    855\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[0;32m--> 856\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m        \u001b[49m\u001b[43mremaining_retries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mremaining_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/ml_framework_311/lib/python3.11/site-packages/openai/_base_client.py:908\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    905\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mis_closed:\n\u001b[1;32m    906\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m--> 908\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    909\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m httpx\u001b[38;5;241m.\u001b[39mTimeoutException \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    910\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mAuthenticationError\u001b[0m: Error code: 401 - {'error': {'message': 'Incorrect API key provided: sk-pGOO6***************************************OhIK. You can find your API key at https://platform.openai.com/account/api-keys.', 'type': 'invalid_request_error', 'param': None, 'code': 'invalid_api_key'}}"
     ]
    }
   ],
   "source": [
    "'''\n",
    "MultiHopQA \n",
    "'''\n",
    "\n",
    "\n",
    "from dataset import MultiHopQADataset\n",
    "import json\n",
    "not_json_mode_path = '/Users/arnaudstiegler/back-office-llm-bench/openai_predictions/multi_hop_qa/predictions_not_json_mode.json'\n",
    "json_mode_path = '/Users/arnaudstiegler/back-office-llm-bench/openai_predictions/multi_hop_qa/predictions_json_mode.json'\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "SYS_PROMPT = (\n",
    "    \"You are an AI agent used for automation. Do not act like a chatbot. Execute the task and\"\n",
    "    \"follow the instructions for the formatting of the output.\"\n",
    ")\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "json_mode = True\n",
    "preds = json.load(open(json_mode_path if json_mode else not_json_mode_path))\n",
    "dataset = MultiHopQADataset(json_mode=False)\n",
    "contains_json = []\n",
    "correct_json = []\n",
    "correct_answer = []\n",
    "for i in range(len(preds)):\n",
    "    answer = preds[i]['choices'][0]['message']['content']\n",
    "    sample = preds[i]['sample']\n",
    "\n",
    "    json_dict = find_and_parse_json(answer)\n",
    "\n",
    "    if not json_dict:\n",
    "        contains_json.append(0)\n",
    "        print(answer)\n",
    "        print('======')\n",
    "    else:\n",
    "        contains_json.append(1)\n",
    "\n",
    "        # if not set(json_dict.keys()) == {'answer'}:\n",
    "        if not set(json_dict.keys()) == {\"answer\"}:\n",
    "            correct_json.append(0)\n",
    "        else:\n",
    "            correct_json.append(1)\n",
    "\n",
    "            prompt = f'first answer: {sample[\"answer\"]}, second answer: {json_dict[\"answer\"]}. Are those 2 answers refering to the same thing? Answer either yes or no and nothing else. Just yes or no'\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "                    {\"role\": \"user\", \"content\": prompt},\n",
    "                ],\n",
    "                # response_format={\"type\": \"json_object\"},\n",
    "            )\n",
    "            if response.choices[0].message.content.lower() == 'yes':\n",
    "                correct_answer.append(1)\n",
    "            elif response.choices[0].message.content.lower() == 'no':\n",
    "                correct_answer.append(0)\n",
    "            else:\n",
    "                import ipdb; ipdb.set_trace()\n",
    "    \n",
    "\n",
    "print('contains parsable json: ', sum(contains_json) / len(contains_json))\n",
    "print('json dict has the valid format: ', sum(correct_json) / len(correct_json))\n",
    "print('accuracy on the valid answers: ', sum(correct_answer) / len(correct_answer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b375fa6b-9538-4da4-aec4-d20432a94c52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'first answer: to protect lives and properties in conjunction with Nigeria police, second answer: Protection of lives and properties. Are those 2 answers refering to the same thing? Answer either yes or no and nothing else. Just yes or no'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f8d7921f-4e3f-4812-8a32-312629f9f1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# answer=\"{'effective_date': '2001-04-18', 'jurisdiction': 'Oregon', 'party': ['Eric_Dean_Sprunk', 'Nike_Inc.']}\"\n",
    "\n",
    "prompt = '''\n",
    "The text above is the transcription of an NDA. \n",
    "\n",
    "# Extraction:\n",
    "There are up to 6 attributes to be extracted from the transcription:\n",
    "\n",
    "effective_date - date in YYYY-MM-DD format, at which point the contract is legally binding,\n",
    "jurisdiction - under which state or country jurisdiction is the contract signed,\n",
    "party - party or parties of the contract,\n",
    "term - length of the legal contract as expressed in the document.\n",
    "Note that party usually occur more than once.\n",
    "\n",
    "# Normalization:\n",
    "The expected pieces of information were normalized to some degree:\n",
    "\n",
    "in attribute values, all spaces   and colons : were replaced with an underscores _,\n",
    "all expected dates should be returned in YYYY-MM-DD format,\n",
    "values for attribute term are normalized with the same original units e.g. eleven months is changed to 11_months; all of them are in the same format: {number}_{units}.\n",
    "For jurisdiction, only return the state name and nothing else. For instance, return \"California\" instead of \"State_of_California\"\n",
    "\n",
    "# Output Format:\n",
    "The output has to be a valid json with the following format:\n",
    "{\"effective_date\": \"value\", \"jurisdiction\":\"value\", \"party\":[\"value_1\", \"value_2\", ...], \"term\":\"value\"}\n",
    "\n",
    "So for instance:\n",
    "\"{'effective_date': '2020-01-12', 'jurisdiction': 'Utah', 'party': ['Bill_Gates', 'Coca_Cola_Inc.']}\"\n",
    "\n",
    "You can reason and explain your choices before returning the JSON object, you just have to have that json object in the output\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3a0606a0-2ccd-4769-99dd-0a762217c6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import MultiHopQADataset\n",
    "\n",
    "dataset = MultiHopQADataset(json_mode=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b762c95d-ccc9-48ac-8ed0-67180710854c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "SYS_PROMPT = (\n",
    "    \"You are an AI agent used for automation. Do not act like a chatbot. Execute the task and\"\n",
    "    \"follow the instructions for the formatting of the output as a JSON object.\"\n",
    ")\n",
    "client = OpenAI(api_key=api_key)\n",
    "response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": SYS_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": dataset[1].prompt},\n",
    "        ],\n",
    "        # response_format={\"type\": \"json_object\"},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9b3460cb-c363-419c-8eed-2f73babeec9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'answer': 'Not applicable'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_and_parse_json(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "235133e5-68be-41e6-9621-9f15c17f8823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- The group that Bush declared war against is Al-Qaeda.\\n- The country with the world\\'s largest economy in 2014 was the United States.\\n- The biggest terrorist attack by Al-Qaeda in the United States in 2014 was not applicable as there were no major attacks by Al-Qaeda in the United States during that year.\\n\\n```json\\n{\"answer\": \"Not applicable\"}\\n``` '"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "210ebcb8-f160-4568-a9af-7433ad7b598e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the 9/11 attacks'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1].answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d26ad2a6-1828-4645-9d49-848a6a3615fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What were the biggest terrorist attacks by the group Bush declared war against in the country with the world\\'s largest economy in 2014?  \\n  \\n        Answer the question and return a JSON object with a key \"answer\" and the value being the answer\\n        to the question. Do not put any explanation of any sort in the Json object, only the actual\\n        answer to the question.\\n        \\n        For instance, if the question is: what is the capital of the country with the largest GDP in the world?\\n        The output would be:\\n        {\"answer\": \"USA\"}\\n        Typically, the answer should only be a few words and not a sentence.\\n        \\n         Think step by step. First write the reasoning to get to the answer, then write the Json object containing only the answer'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[1].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e87cfe5d-07d1-4319-a630-b7343d6bb35e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['multihop', 'meta_info', 'final_answer', 'question', 'answer', 'tag', 'sub_questions'])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 'multihop': True\n",
    "# 'question': 'What is the dosage for Advil?',\n",
    "# 'unanswerable': None\n",
    "# 'answer': 'Molotov -- Ribbentrop Pact of 1939'\n",
    "sample.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "92bdf597-77a9-4039-9da1-674369e119bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [sample for sample in dataset['train'] if sample['answer']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "57906ed8-9baf-4cfe-8882-1625dd25fcea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6976"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Reason and answer the question above.\n",
    "The return should be a json object, with one key \"answer\" and the corresponding value should be the answer to the question.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "c2b6d83f-ea12-480b-a62a-c33435e59f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'multihop': True,\n",
       " 'meta_info': {'attribute_1': None,\n",
       "  'attribute_2': None,\n",
       "  'category': None,\n",
       "  'comparison_attribute': None,\n",
       "  'entity': None,\n",
       "  'entity_1': None,\n",
       "  'entity_2': None,\n",
       "  'index': None,\n",
       "  'list_of_attributes': None,\n",
       "  'llm': 'wizard_lm',\n",
       "  'negatives': None,\n",
       "  'selected': None,\n",
       "  'slot_dic': None,\n",
       "  'src': 'un_musicque'},\n",
       " 'final_answer': 'Summary: The knowledge provided does not contain any information about the expansion of a single reed\\'s role or any composer who might have done so. There is no mention of any piece of music that is used as a cliché to convey refinement in relation to this topic. The knowledge only pertains to Thomas & Friends and Far East Movement\\'s song \"The Illest.\" Without any information about the composer or the piece of music in question, it is impossible to generate a reasoning to answer this question. The knowledge provided does not offer any context or hints that could lead to a potential answer.\\nAnswer: I\\'m sorry, but I cannot answer this question based on the knowledge provided.',\n",
       " 'question': \"What piece, by the composer who expanded the single reeds' role, is used as a cliché to convey refinement?\",\n",
       " 'answer': 'Eine kleine Nachtmusik',\n",
       " 'tag': 'un_musique-train.json',\n",
       " 'sub_questions': [{'answer': None,\n",
       "   'extra_info': None,\n",
       "   'long_answer': 'The question is about the expansion of a single reed\\'s role, which seems to be unrelated to the information provided about Thomas & Friends and Far East Movement\\'s song \"The Illest.\" There is no direct connection or contextual hint in the given knowledge that would allow me to answer this question.\\nAnswer: I\\'m sorry, but I cannot answer this question based on the knowledge provided.',\n",
       "   'paragraph': 'Thomas & Friends. Originally, narrating was used as the only voice in the series until 2008. Britt Allcroft thought it essential to convey the episode as a story that would be read from a book at home. Individual voice - over actors were given to both the UK and US dubs of the Series, following the switch to full CGI animation in 2009. The Illest. \"The Illest\" is a song by American hip hop group Far East Movement. The song was co-written and produced by Norwegian music producer and songwriter Axident and Wallpaper. frontman Ricky Reed, and features a guest appearance from American rapper Riff Raff. It was released digitally as a single on July 2, 2013 and has since peaked at number 18 on the US \"Billboard\" Hot Rap Songs. \"The Illest\" was featured as a bonus track and a single from the 2013 Special Edition release of their fourth studio album \"Dirty Bass\" (becoming the album\\'s fifth single overall). ',\n",
       "   'question': 'Who expanded the single reeds role?',\n",
       "   'unanswerable': True}]}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "5e9dcbc0-f902-465c-947a-c86bde98decc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"What piece, by the composer who expanded the single reeds' role, is used as a cliché to convey refinement?\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[3]['question']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53385292-5376-4b5f-9f02-d4ff3a92ba7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
